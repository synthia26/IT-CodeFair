# -*- coding: utf-8 -*-
"""nt-tourism-prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MrqDWEHxTUaU7IU7U5ArKKuy314x_nbJ
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install -q hvplot

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import hvplot.pandas
from scipy import stats

# %matplotlib inline
sns.set_style("whitegrid")
plt.style.use("fivethirtyeight")

# Load the Tourist dataset
data = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Govhack-data/Tourist-Dataset.csv')
data

# Check basic information about the dataset
data.info()

# Check number of rows and columns in dataset
data.shape

# Get Statistical measures of the dataset
data.describe()

# Check if there are any duplicates in dataset
data.duplicated()

# Check if there is any missing data in dataset
missing = data.isnull().sum()
missing

# Tourist Prediction Data Analysis
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.metrics import mean_squared_error

# Calculate total visitors (domestic + international)
data['total_visitors'] = data['vis-dom'] + data['vis-int']

# Calculate the year-over-year change in total visitors (increase or decrease)
data['visitor_change'] = data['total_visitors'].diff()

# Correlation analysis for feature selection
plt.figure(figsize=(10, 8))
sns.heatmap(data[features + ['visitor_change']].corr(), annot=True, cmap='coolwarm')
plt.title('Feature Correlation with visitor_change')
plt.show()

# Drop the rows with missing values in the 'visitor_change' column (the first row will have NaN)
data = data.dropna(subset=['visitor_change'])

# Define a more extensive feature set by including other relevant columns
features = ['total_visitors', 'vis-night-dom', 'vis-night-int', 'exp-dom', 'exp-int',
            'avg_length-stay-dom', 'avg_length-stay-int', 'avg-spend-trip-dom', 'avg-spending-trip-int']

X = data[features]  # Using multiple features now
y = data['visitor_change']

# Standardizing the data by scaling it
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42, shuffle=False)

# Initialize the Gradient Boosting Regressor model
model = GradientBoostingRegressor(n_estimators=100, random_state=42)

# Train the model on the training data
model.fit(X_train, y_train)

# Make predictions on the test data
y_pred = model.predict(X_test)

# Evaluate the model using Mean Squared Error
mse = mean_squared_error(y_test, y_pred)

# Display results
print("Mean Squared Error on test data:", mse)

# Display actual vs predicted results
result_df = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})
print(result_df.head())